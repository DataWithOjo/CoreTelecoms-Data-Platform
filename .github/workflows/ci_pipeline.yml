name: CoreTelecoms CI Pipeline

on:
  pull_request:
    branches: [ "main" ]
    paths-ignore:
      - '**.md'
      - 'docs/**'
  push:
    branches: [ "**" ]
    paths-ignore:
      - '**.md'
      - 'docs/**'

jobs:
  quality-checks:
    name: Code Quality & Linting
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v3

      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install Linting Tools
        run: pip install flake8 sqlfluff sqlfluff-templater-dbt dbt-snowflake

      - name: Lint Python Code (DAGs & Scripts)
        run: |
          flake8 dags scripts --count --select=E9,F63,F7,F82 --show-source --statistics

      - name: Lint SQL Code (dbt Models)
        run: |
          # Dynamically find the dbt project folder
          DBT_DIR=$(dirname $(find . -name dbt_project.yml -type f | head -n 1))
          echo "Found dbt project at: $DBT_DIR"
          
          if [ -z "$DBT_DIR" ]; then
            echo "Error: dbt_project.yml not found!"
            exit 1
          fi

          # Configure sqlfluff for OFFLINE mode (Jinja templater)
          # We create a config file that treats dbt macros as built-ins but doesn't connect to DB
          cat <<EOF > .sqlfluff
          [sqlfluff]
          dialect = snowflake
          templater = jinja
          runaway_limit = 10
          max_line_length = 120
          exclude_rules = ST06, LT05, CP01

          [sqlfluff:templater:jinja]
          apply_dbt_builtins = True
          EOF

          # Run sqlfluff using the generated config
          sqlfluff lint "$DBT_DIR/models" --config .sqlfluff

  unit-tests:
    name: Unit Testing
    runs-on: ubuntu-latest
    needs: quality-checks
    steps:
      - name: Checkout Code
        uses: actions/checkout@v3

      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install Dependencies
        run: |
          pip install apache-airflow pytest
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Run Airflow DAG Unit Tests
        env:
          AIRFLOW_HOME: ${{ github.workspace }}
        run: |
          export PYTHONPATH="${PYTHONPATH}:${{ github.workspace }}"
          
          pytest tests/test_load_snowflake_pipeline.py